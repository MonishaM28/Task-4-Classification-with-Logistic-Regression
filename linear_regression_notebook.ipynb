{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e0ba76b",
   "metadata": {},
   "source": [
    "# Linear Regression â€” Standout Implementation\n",
    "\n",
    "**Objective:** Implement simple & multiple linear regression on a housing dataset, do advanced diagnostics, and produce a professional README-quality notebook suitable for GitHub.\n",
    "\n",
    "**How to use:** Download the housing dataset (e.g., `housing.csv`) and place it in the same folder as this notebook. If you use Kaggle, download and rename the file to `housing.csv` or `dataset.csv`.\n",
    "\n",
    "**Contents:**\n",
    "- EDA & preprocessing\n",
    "- Feature engineering & selection\n",
    "- Simple and multiple linear regression (scikit-learn)\n",
    "- Statsmodels OLS summary for statistical insight\n",
    "- Regularization (Ridge, Lasso, ElasticNet)\n",
    "- Multicollinearity check (VIF)\n",
    "- Diagnostics & plots (residuals, Q-Q)\n",
    "- Model interpretability (coefficients, SHAP fallback)\n",
    "- Save model with `joblib`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a75e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from joblib import dump\n",
    "\n",
    "print('Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f4f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset - update filename if needed\n",
    "for fname in ('housing.csv', 'dataset.csv', 'data.csv'):\n",
    "    if os.path.exists(fname):\n",
    "        DATA_PATH = fname\n",
    "        break\n",
    "else:\n",
    "    DATA_PATH = None\n",
    "\n",
    "if DATA_PATH is None:\n",
    "    print('No dataset found in working directory.\\nPlease download the housing dataset (Kaggle link provided in the assignment) and place it as `housing.csv` here.')\n",
    "else:\n",
    "    print('Loading dataset from', DATA_PATH)\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print('Shape:', df.shape)\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7065ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic EDA (run after loading the dataset)\n",
    "try:\n",
    "    df\n",
    "except NameError:\n",
    "    raise RuntimeError('Dataset not loaded. Run the dataset cell and ensure file is present.')\n",
    "\n",
    "print('\\n--- Data Info ---')\n",
    "df.info()\n",
    "\n",
    "print('\\n--- Missing values (per column) ---')\n",
    "print(df.isnull().sum().sort_values(ascending=False).head(20))\n",
    "\n",
    "print('\\n--- Descriptive statistics ---')\n",
    "display(df.describe().T)\n",
    "\n",
    "# Target distribution check (assumes target named price or SalePrice - adjust if different)\n",
    "possible_targets = [c for c in df.columns if 'price' in c.lower() or 'saleprice' in c.lower() or c.lower()=='price']\n",
    "print('\\nPossible target columns detected:', possible_targets)\n",
    "\n",
    "if possible_targets:\n",
    "    target = possible_targets[0]\n",
    "else:\n",
    "    # fallback to last numeric column\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    target = numeric_cols[-1]\n",
    "    print('No obvious \"price\" column found; using numeric column', target)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df[target].dropna(), kde=True)\n",
    "plt.title(f'Distribution of target: {target}')\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap for numeric features (top 20)\n",
    "num_df = df.select_dtypes(include=[np.number])\n",
    "if num_df.shape[1] > 1:\n",
    "    corr = num_df.corr()[target].abs().sort_values(ascending=False).head(20)\n",
    "    print('\\nTop correlated numeric features with target:')\n",
    "    display(corr)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(num_df[corr.index].corr(), annot=False, cmap='coolwarm')\n",
    "    plt.title('Correlation matrix (top numeric features)')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Not enough numeric columns for correlation heatmap.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6bbc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering suggestions (customize for your dataset)\n",
    "# Example transformations - comment/uncomment based on dataset columns\n",
    "\n",
    "# If dataset has 'sqft' and 'price', create price_per_sqft\n",
    "def safe_add_price_per_sqft(df):\n",
    "    candidates = [c for c in df.columns if 'sqft' in c.lower() or 'square' in c.lower()]\n",
    "    if candidates and target in df.columns:\n",
    "        col = candidates[0]\n",
    "        df['price_per_sqft'] = df[target] / (df[col].replace(0, np.nan))\n",
    "        print('Added price_per_sqft using', col)\n",
    "    else:\n",
    "        print('No sqft-like column detected; skipping price_per_sqft feature')\n",
    "\n",
    "safe_add_price_per_sqft(df)\n",
    "\n",
    "# Example: convert date column to year or age\n",
    "date_cols = [c for c in df.columns if 'yr' in c.lower() or 'year' in c.lower() or 'date' in c.lower()]\n",
    "if date_cols:\n",
    "    print('Date-like columns:', date_cols)\n",
    "    # Uncomment and customize as needed\n",
    "    # df['age'] = 2025 - df[date_cols[0]]\n",
    "\n",
    "print('\\nAfter feature suggestions, df.columns sample:')\n",
    "print(df.columns[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf69016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling pipeline (automated preprocessing)\n",
    "# This pipeline will:\n",
    "# - Separate numerical and categorical features\n",
    "# - Impute missing values\n",
    "# - Scale numeric features\n",
    "# - One-hot encode categorical features\n",
    "\n",
    "# Select features automatically: numeric + top-k categorical by cardinality\n",
    "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if target in numeric_features:\n",
    "    numeric_features.remove(target)\n",
    "\n",
    "categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print('Numeric features:', len(numeric_features))\n",
    "print('Categorical features:', len(categorical_features))\n",
    "\n",
    "# Define transformers\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Prepare X and y\n",
    "X = df[numeric_features + categorical_features].copy()\n",
    "y = df[target].copy()\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Simple Linear Regression pipeline (as baseline)\n",
    "simple_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', LinearRegression())])\n",
    "\n",
    "print('Fitting baseline LinearRegression...')\n",
    "simple_pipeline.fit(X_train, y_train)\n",
    "print('Baseline fitted')\n",
    "\n",
    "# Predictions & metrics\n",
    "y_pred = simple_pipeline.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Baseline results -- MAE: {mae:.4f}, RMSE: {rmse:.4f}, R2: {r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb0772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statsmodels OLS: get design matrix after preprocessing (useful for p-values)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Fit preprocessor on full X and transform to a dense DataFrame\n",
    "preprocessor.fit(X)\n",
    "\n",
    "# Get transformed feature names\n",
    "num_cols = numeric_features\n",
    "cat_cols = []\n",
    "if categorical_features:\n",
    "    ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "    ohe_features = ohe.get_feature_names_out(categorical_features)\n",
    "    cat_cols = list(ohe_features)\n",
    "\n",
    "feature_names = num_cols + cat_cols\n",
    "\n",
    "X_trans = preprocessor.transform(X)\n",
    "# Convert to DataFrame (handle sparse matrix if produced)\n",
    "if hasattr(X_trans, 'toarray'):\n",
    "    X_arr = X_trans.toarray()\n",
    "else:\n",
    "    X_arr = X_trans\n",
    "X_design = pd.DataFrame(X_arr, columns=feature_names)\n",
    "X_design = sm.add_constant(X_design)\n",
    "\n",
    "print('Design matrix shape for statsmodels:', X_design.shape)\n",
    "\n",
    "# Fit OLS (may be slow for large matrices)\n",
    "ols_model = sm.OLS(y, X_design).fit()\n",
    "print(ols_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance Inflation Factor (VIF) calculation\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Compute VIF on numeric part only (after imputation/scaling)\n",
    "X_num = preprocessor.named_transformers_['num'].transform(df[numeric_features])\n",
    "X_num_df = pd.DataFrame(X_num, columns=numeric_features)\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = X_num_df.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X_num_df.values, i) for i in range(X_num_df.shape[1])]\n",
    "\n",
    "print('VIF for numeric features:')\n",
    "display(vif_data.sort_values('VIF', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dbd67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized regression: Ridge, Lasso, ElasticNet (with cross-validation recommended)\n",
    "models = {\n",
    "    'Linear': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Lasso': Lasso(alpha=0.1),\n",
    "    'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_test)\n",
    "    results[name] = {\n",
    "        'MAE': mean_absolute_error(y_test, preds),\n",
    "        'MSE': mean_squared_error(y_test, preds),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, preds)),\n",
    "        'R2': r2_score(y_test, preds)\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print('Model comparison:')\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7955fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficients from linear model after preprocessing\n",
    "coef_pipeline = simple_pipeline\n",
    "pre = coef_pipeline.named_steps['preprocessor']\n",
    "reg = coef_pipeline.named_steps['regressor']\n",
    "\n",
    "# Build feature names again\n",
    "num_cols = numeric_features\n",
    "cat_cols = []\n",
    "if categorical_features:\n",
    "    ohe = pre.named_transformers_['cat'].named_steps['onehot']\n",
    "    cat_cols = list(ohe.get_feature_names_out(categorical_features))\n",
    "\n",
    "all_feat_names = num_cols + cat_cols\n",
    "coefs = reg.coef_\n",
    "\n",
    "coef_df = pd.DataFrame({'feature': all_feat_names, 'coefficient': coefs})\n",
    "coef_df = coef_df.reindex(coef_df.coefficient.abs().sort_values(ascending=False).index)\n",
    "\n",
    "print('Top coefficients:')\n",
    "display(coef_df.head(20))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(data=coef_df.head(20), x='coefficient', y='feature')\n",
    "plt.title('Top 20 feature coefficients (baseline linear regression)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b49a28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual diagnostics\n",
    "preds = simple_pipeline.predict(X_test)\n",
    "residuals = y_test - preds\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.scatterplot(x=preds, y=residuals)\n",
    "plt.axhline(0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Predicted')\n",
    "plt.show()\n",
    "\n",
    "# Q-Q plot for residuals\n",
    "sm.qqplot(residuals, line='s')\n",
    "plt.title('Q-Q plot of residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef01d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP - optional (if shap is installed). Provides local & global interpretability.\n",
    "try:\n",
    "    import shap\n",
    "    # For pipeline, explain using transformed features may be needed\n",
    "    X_train_trans = preprocessor.transform(X_train)\n",
    "    explainer = shap.Explainer(simple_pipeline.named_steps['regressor'], X_train_trans)\n",
    "    X_test_trans = preprocessor.transform(X_test)\n",
    "    shap_values = explainer(X_test_trans)\n",
    "    print('SHAP available â€” showing summary plot (may be slow)')\n",
    "    shap.summary_plot(shap_values, features=X_test_trans, feature_names=(num_cols + cat_cols))\n",
    "except Exception as e:\n",
    "    print('SHAP not available or failed to run:', e)\n",
    "    print('You can install shap via pip to enable this section: pip install shap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff658197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the baseline model for deployment\n",
    "os.makedirs('saved_model', exist_ok=True)\n",
    "dump(simple_pipeline, 'saved_model/linear_model_baseline.joblib')\n",
    "print('Saved baseline model to saved_model/linear_model_baseline.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8217384",
   "metadata": {},
   "source": [
    "## Next steps & suggestions for the README\n",
    "\n",
    "- Hyperparameter tuning for Ridge/Lasso/ElasticNet using `GridSearchCV` or `RandomizedSearchCV`.\n",
    "- Try tree-based models (RandomForest, XGBoost) as benchmarks and compare performance.\n",
    "- Use MLflow or Weights & Biases to track experiments and metrics.\n",
    "- Deploy a Streamlit app (`streamlit run app.py`) that loads the saved model and offers a prediction form.\n",
    "\n",
    "---\n",
    "\n",
    "**When submitting to GitHub:** include `README.md`, the notebook `linear_regression_notebook.ipynb`, `saved_model/linear_model_baseline.joblib` (or provide link to model), and a sample `dataset.csv` or instructions to download it from Kaggle.\n",
    "\n",
    "Good luck! If you'd like, I can also: \n",
    "- generate the `README.md` for the repository,\n",
    "- create a Streamlit app skeleton to deploy the model,\n",
    "- or run this notebook here if you upload the dataset file.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
